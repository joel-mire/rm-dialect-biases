{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr2/jmire/rm-dialect-biases/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import yaml\n",
    "import os\n",
    "from constants import *\n",
    "from utils.pandas_utils import save_df\n",
    "from utils import pandas_utils\n",
    "from code_detection_strategy.GPT4oCodeDetectionStrategy import GPT4oCodeDetectionStrategy\n",
    "from translation_strategy.ValueTranslationStrategy import ValueTranslationStrategy\n",
    "from translation_strategy.PhonateTranslationStrategy import PhonateTranslationStrategy\n",
    "import subprocess\n",
    "from functools import lru_cache\n",
    "from utils import idp_utils, langid_utils, config_utils\n",
    "import json\n",
    "import random\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load\n",
    "import torch\n",
    "import gc\n",
    "import concurrent.futures\n",
    "import math\n",
    "from langDialect.getDialect import detectDialect\n",
    "from tabulate import tabulate\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Assemble Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2985, 5)\n"
     ]
    }
   ],
   "source": [
    "HAVE_ALL_DG_DATASETS = config_utils.have_all_datasets()\n",
    "\n",
    "if HAVE_ALL_DG_DATASETS:\n",
    "    raw_groenwold_wme_df = pd.read_json(path_or_buf=RAW_GROENWOLD_WME_PATH, \n",
    "                                        orient='records',\n",
    "                                        lines=True)\n",
    "    raw_groenwold_aal_df = pd.read_json(path_or_buf=RAW_GROENWOLD_AAL_PATH, \n",
    "                                        orient='records', \n",
    "                                        lines=True)\n",
    "    raw_deas_prompts_df = pd.read_csv(RAW_DEAS_PROMPTS_PATH)\n",
    "    raw_deas_eval_df = pd.read_csv(RAW_DEAS_EVAL_PATH)\n",
    "\n",
    "    # Build DeasGroenwold (DG) df\n",
    "    groenwold_data = pd.DataFrame({\n",
    "        'source': ['groenwold'] * len(raw_groenwold_wme_df),\n",
    "        'txt_wme': raw_groenwold_wme_df['text'].values,\n",
    "        'txt_aal': raw_groenwold_aal_df['text'].values,\n",
    "    })\n",
    "    deas_prompts_data = pd.DataFrame({\n",
    "        'source': ['deas'] * len(raw_deas_prompts_df),\n",
    "        'txt_wme': raw_deas_prompts_df['wme_text'].values,\n",
    "        'txt_aal': raw_deas_prompts_df['aal_text'].values\n",
    "    })\n",
    "    deas_eval_data = pd.DataFrame({\n",
    "        'source': ['deas'] * len(raw_deas_eval_df),\n",
    "        'txt_wme': raw_deas_eval_df['wme_text'].values,\n",
    "        'txt_aal': raw_deas_eval_df['aal_text'].values\n",
    "    })\n",
    "    dg_df = pd.concat([groenwold_data, deas_prompts_data, deas_eval_data], ignore_index=True)\n",
    "    dg_df['txt_empty'] = [''] * len(dg_df)\n",
    "    save_df(dg_df, DEAS_GROENWOLD_PATH)\n",
    "    print(dg_df.shape)\n",
    "\n",
    "raw_rb_dataset_dict = datasets.load_dataset(REWARD_BENCH_DATASET_NAME)\n",
    "raw_rb_df = pd.DataFrame.from_dict(data=raw_rb_dataset_dict['filtered'].to_dict())\n",
    "\n",
    "# Build RewardBench (RB) df\n",
    "rb_df = pd.DataFrame({\n",
    "    'subset': raw_rb_df['subset'],\n",
    "    'txt_prompt_orig': raw_rb_df[PROMPT_KEY],\n",
    "    'txt_chos_orig': raw_rb_df[CHOSEN_KEY],\n",
    "    'txt_rej_orig': raw_rb_df[REJECTED_KEY],\n",
    "    'txt_empty': [''] * len(raw_rb_df)\n",
    "})\n",
    "save_df(rb_df, REWARD_BENCH_PATH)\n",
    "print(rb_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out examples containing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfiltered rb length 2985\n",
      "filtered rb length 1843\n"
     ]
    }
   ],
   "source": [
    "# Filter out datapoints containing code\n",
    "code_detection_strategy = GPT4oCodeDetectionStrategy()\n",
    "\n",
    "if HAVE_ALL_DG_DATASETS:\n",
    "  dg_code_predictions_path = f'{CODE_PREDICTIONS_DIR}/dg.csv'\n",
    "  dg_df = code_detection_strategy.detect(df=dg_df, \n",
    "                                        results_path=dg_code_predictions_path,\n",
    "                                        cols=['txt_wme'])\n",
    "  print(\"unfiltered dg length\", len(dg_df))\n",
    "  dg_df = dg_df[dg_df[GPT4O_CODE_PREDICTION_COL] == False]\n",
    "  dg_df = dg_df.reset_index()\n",
    "  save_df(dg_df, DEAS_GROENWOLD_PATH)\n",
    "  print(\"filtered dg length\", len(dg_df))\n",
    "\n",
    "rb_code_predictions_path = f'{CODE_PREDICTIONS_DIR}/rb.csv'\n",
    "rb_df = code_detection_strategy.detect(df=rb_df, \n",
    "                                       results_path=rb_code_predictions_path,\n",
    "                                       cols=['txt_prompt_orig', 'txt_chos_orig', 'txt_rej_orig'])\n",
    "print(\"unfiltered rb length\", len(rb_df))\n",
    "rb_df = rb_df[rb_df[GPT4O_CODE_PREDICTION_COL] == False]\n",
    "rb_df = rb_df.reset_index()\n",
    "save_df(rb_df, REWARD_BENCH_PATH)\n",
    "print(\"filtered rb length\", len(rb_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add VALUE AAL Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_translation_strategy = ValueTranslationStrategy()\n",
    "\n",
    "if HAVE_ALL_DG_DATASETS and GET_ALL_RESULTS:\n",
    "  dg_df = value_translation_strategy.translate(dg_df, DG_DATASET_NAME, ['txt_wme'], [VALUE_AAL_SUFFIX], FORCE_REBUILD_VALUE)\n",
    "  save_df(dg_df, DEAS_GROENWOLD_PATH)\n",
    "\n",
    "rb_df = value_translation_strategy.translate(rb_df, RB_DATASET_NAME, ['txt_prompt_orig', 'txt_chos_orig', 'txt_rej_orig'], [VALUE_AAL_SUFFIX], FORCE_REBUILD_VALUE)\n",
    "save_df(rb_df, REWARD_BENCH_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add PhonATe AAL Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached results for translation from txt_prompt_vAAL -> txt_prompt_vpAAL\n",
      "Using cached results for translation from txt_chos_vAAL -> txt_chos_vpAAL\n",
      "Using cached results for translation from txt_rej_vAAL -> txt_rej_vpAAL\n"
     ]
    }
   ],
   "source": [
    "phonate_translation_strategy = PhonateTranslationStrategy()\n",
    "\n",
    "if HAVE_ALL_DG_DATASETS and GET_ALL_RESULTS:\n",
    "  dg_df = phonate_translation_strategy.translate(dg_df,\n",
    "                                                 DG_DATASET_NAME, \n",
    "                                                 input_cols=[f'txt_{VALUE_AAL_SUFFIX}'],\n",
    "                                                 output_col_suffixes=[f'{VALUE_AAL_PHONATE_SUFFIX}'],\n",
    "                                                 force_retranslate=FORCE_REBUILD_PHONATE)\n",
    "  save_df(dg_df, DEAS_GROENWOLD_PATH)\n",
    "\n",
    "rb_df = phonate_translation_strategy.translate(rb_df, \n",
    "                                               RB_DATASET_NAME, \n",
    "                                               input_cols=[f'txt_prompt_{VALUE_AAL_SUFFIX}', f'txt_chos_{VALUE_AAL_SUFFIX}', f'txt_rej_{VALUE_AAL_SUFFIX}'], \n",
    "                                               output_col_suffixes=[f'{VALUE_AAL_PHONATE_SUFFIX}'],\n",
    "                                               force_retranslate=FORCE_REBUILD_PHONATE)\n",
    "save_df(rb_df, REWARD_BENCH_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Model Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_skip_execution(prompt_strategy, base_suffix, cur_suffix):\n",
    "  if prompt_strategy in [W_BASE_PROMPT_MOD_CANDIDATE_KEY, W_MOD_PROMPT_BASE_CANDIDATE_KEY]:\n",
    "    return base_suffix == cur_suffix\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def get_prompt_and_candidate_suffixes(prompt_strategy, base_suffix, cur_suffix):\n",
    "  if prompt_strategy == WO_PROMPT_KEY:\n",
    "    prompt_suffix, candidate_suffix = \"empty\", cur_suffix\n",
    "  elif prompt_strategy == W_PROMPT_KEY:\n",
    "    prompt_suffix, candidate_suffix = cur_suffix, cur_suffix\n",
    "  elif prompt_strategy == W_BASE_PROMPT_MOD_CANDIDATE_KEY:\n",
    "    prompt_suffix, candidate_suffix = base_suffix, cur_suffix\n",
    "  elif prompt_strategy == W_MOD_PROMPT_BASE_CANDIDATE_KEY:\n",
    "    prompt_suffix, candidate_suffix = cur_suffix, base_suffix\n",
    "  return prompt_suffix, candidate_suffix\n",
    "\n",
    "def get_score_col_name(model, prompt_strategy, suffix, candidate_type):\n",
    "   return f'{suffix}${prompt_strategy}${model}${candidate_type}'\n",
    "\n",
    "def get_rb_scores(model_name, input_path, output_dir, force_rerun=False):\n",
    "    \"\"\"Run RewardBench and skip if output exists unless force_rerun is True.\"\"\"\n",
    "    output_path = f'{output_dir}{model_name}_all.jsonl'\n",
    "    if not os.path.exists(output_path) or force_rerun:\n",
    "      param_dict = build_rb_param_dict(model_name, input_path, output_dir)\n",
    "      _stdout, _stderr = invoke_rewardbench(param_dict)\n",
    "    return pd.read_json(output_path, orient='records', lines=True)\n",
    "\n",
    "with open(REWARD_BENCH_EVAL_CONFIGS_PATH, \"r\") as f:\n",
    "  reward_bench_eval_configs = yaml.load(f.read(), Loader=yaml.FullLoader)\n",
    "  \n",
    "def build_rb_param_dict(model_name, input_path, output_dir):\n",
    "  param_dict = {}\n",
    "  model_config = reward_bench_eval_configs[model_name]\n",
    "  param_dict['model'] = model_name\n",
    "  # add ref_model for DPO models\n",
    "  if 'ref_model' in model_config.keys():\n",
    "    param_dict['ref_model'] = model_config['ref_model']\n",
    "  param_dict['tokenizer'] = model_config['tokenizer']\n",
    "  param_dict['dataset'] = input_path\n",
    "  param_dict['chat_template'] = model_config['chat_template']\n",
    "  param_dict['batch_size'] = model_config['batch_size']\n",
    "  param_dict['trust_remote_code'] = model_config['trust_remote_code']\n",
    "  param_dict['output_dir'] = output_dir\n",
    "  param_dict['save_all'] = True\n",
    "  return param_dict\n",
    "\n",
    "def invoke_rewardbench(param_dict):\n",
    "  command_parts = ['rewardbench']\n",
    "  for key, val in param_dict.items():\n",
    "    if val == None:\n",
    "      continue\n",
    "    if type(val) == bool:\n",
    "      if val:\n",
    "        part = f'--{key}'\n",
    "      else:\n",
    "        continue\n",
    "    else:\n",
    "      part = f'--{key}={val}'\n",
    "    command_parts.append(part)\n",
    "  command_parts.append('--load_json')\n",
    "  print(\" \".join(command_parts))\n",
    "\n",
    "  env = os.environ.copy()\n",
    "  env['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES\n",
    "  result = subprocess.run(command_parts, capture_output=True, text=True, env=env)\n",
    "  return result.stdout, result.stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document-level Reward Model Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution for rb w_prompt orig\n",
      "Starting execution for rb w_prompt vpAAL\n",
      "Starting execution for rb w_mod_prompt_base_candidate vpAAL\n",
      "Starting execution for rb w_base_prompt_mod_candidate vpAAL\n"
     ]
    }
   ],
   "source": [
    "DF_DICT = {\n",
    "   RB_DATASET_NAME: rb_df,\n",
    "}\n",
    "if HAVE_ALL_DG_DATASETS:\n",
    "   DF_DICT[DG_DATASET_NAME] = dg_df\n",
    "\n",
    "for rb_execution_config in RB_EXECUTION_CONFIGS:\n",
    "  dataset_name, col_prefix_dict, base_suffix, mod_suffixes, all_suffixes, prompt_strategies = config_utils.parse_rb_execution_config(rb_execution_config)\n",
    "  if dataset_name == DG_DATASET_NAME and not HAVE_ALL_DG_DATASETS:\n",
    "    continue\n",
    "  df = DF_DICT[dataset_name]\n",
    "  \n",
    "  aligned_scores_dict = {}\n",
    "  for suffix in all_suffixes:\n",
    "    for prompt_strategy in prompt_strategies:\n",
    "      if should_skip_execution(prompt_strategy, base_suffix, suffix):\n",
    "        continue\n",
    "      print(\"Starting execution for\", dataset_name, prompt_strategy, suffix)\n",
    "      prompt_suffix, candidate_suffix = get_prompt_and_candidate_suffixes(prompt_strategy, base_suffix, suffix)\n",
    "\n",
    "      rb_input_dict = {}\n",
    "      rb_input_dict['idx'] = df.index\n",
    "      prompt_col_name = \"txt_empty\" if prompt_strategy == WO_PROMPT_KEY else f'{col_prefix_dict[PROMPT_KEY]}_{prompt_suffix}' \n",
    "      rb_input_dict[PROMPT_KEY] = df[prompt_col_name].apply(lambda x: [{\"role\": \"user\", \"content\": x}])\n",
    "      chos_col_name = f'{col_prefix_dict[CHOSEN_KEY]}_{candidate_suffix}' \n",
    "      rb_input_dict[CHOSEN_KEY] = df[chos_col_name]\n",
    "      rej_col_name = f'{col_prefix_dict[REJECTED_KEY]}_{candidate_suffix}' \n",
    "      rb_input_dict[REJECTED_KEY] = df[rej_col_name]\n",
    "      rb_input_df = pd.DataFrame.from_dict(rb_input_dict)\n",
    "\n",
    "      base_dir = f'{RB_SCORING_DIR}/{dataset_name}/{prompt_strategy}/{suffix}/'\n",
    "      input_path = f'{base_dir}input.jsonl'\n",
    "      if not os.path.exists(input_path) or FORCE_RB_RESCORING:\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        rb_input_df.to_json(input_path, lines=True, orient='records')\n",
    "\n",
    "      for model in MODELS_DICT:\n",
    "        output_dir = f'{base_dir}{model}/'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        if not all(get_score_col_name(model, prompt_strategy, suffix, candidate_type) in df.columns for candidate_type in [CHOSEN_KEY, REJECTED_KEY]) or FORCE_RB_RESCORING:\n",
    "          rb_scores_df = get_rb_scores(\n",
    "            model_name=model,\n",
    "            input_path=input_path,\n",
    "            output_dir=output_dir,\n",
    "            force_rerun=FORCE_RB_RESCORING\n",
    "          )\n",
    "\n",
    "        for candidate_type in [CHOSEN_KEY, REJECTED_KEY]:\n",
    "            aligned_scores = [None] * len(df)\n",
    "            for idx, rb_score in zip(rb_input_df['idx'], rb_scores_df[candidate_type]):\n",
    "                aligned_scores[idx] = rb_score[0] if isinstance(rb_score, list) else rb_score\n",
    "\n",
    "            col_name = f'{suffix}${prompt_strategy}${model}${candidate_type}'\n",
    "            aligned_scores_dict[col_name] = aligned_scores\n",
    "\n",
    "  new_cols_df = pd.DataFrame(aligned_scores_dict)\n",
    "\n",
    "  df = pd.concat([df, new_cols_df], axis=1)\n",
    "  DF_DICT[dataset_name] = df\n",
    "  if dataset_name == DG_DATASET_NAME:\n",
    "      dg_df = df\n",
    "  elif dataset_name == RB_DATASET_NAME:\n",
    "      rb_df = df\n",
    "\n",
    "if HAVE_ALL_DG_DATASETS:\n",
    "  save_df(dg_df, DEAS_GROENWOLD_PATH)\n",
    "save_df(rb_df, REWARD_BENCH_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation: token chosenness x token AALness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GET_ALL_RESULTS:\n",
    "    for rb_execution_config in RB_EXECUTION_CONFIGS:\n",
    "        dataset_name, col_prefix_dict, base_suffix, mod_suffixes, all_suffixes, _ = config_utils.parse_rb_execution_config(rb_execution_config)\n",
    "        prompt_strategy = WO_PROMPT_KEY if dataset_name == DG_DATASET_NAME else W_PROMPT_KEY\n",
    "        col_prefixes = [CHOSEN_KEY] if dataset_name == DG_DATASET_NAME else [CHOSEN_KEY, REJECTED_KEY]\n",
    "        if dataset_name == DG_DATASET_NAME and not HAVE_ALL_DG_DATASETS:\n",
    "            continue\n",
    "        df = DF_DICT[dataset_name]\n",
    "\n",
    "        _, base_candidate_suffix = get_prompt_and_candidate_suffixes(prompt_strategy, base_suffix, base_suffix)\n",
    "        base_txt_cols = [f'{col_prefix_dict[col_prefix]}_{base_candidate_suffix}' for col_prefix in col_prefixes]\n",
    "\n",
    "        for suffix in mod_suffixes:\n",
    "            _, mod_candidate_suffix = get_prompt_and_candidate_suffixes(prompt_strategy, base_suffix, suffix)\n",
    "            mod_txt_cols = [f'{col_prefix_dict[col_prefix]}_{mod_candidate_suffix}' for col_prefix in col_prefixes]\n",
    "            for model in MODELS_DICT:\n",
    "                model_txt_dict = defaultdict(list)\n",
    "                for base_txt_col, mod_txt_col in zip(base_txt_cols, mod_txt_cols):\n",
    "                    base_txt_score_col = pandas_utils.get_corresponding_score_col(base_txt_col, prompt_strategy, model, dataset_name)\n",
    "                    mod_txt_score_col = pandas_utils.get_corresponding_score_col(mod_txt_col, prompt_strategy, model, dataset_name)\n",
    "\n",
    "                    # Vectorized operations to compare scores\n",
    "                    base_txt_scores = df[base_txt_score_col].values\n",
    "                    mod_txt_scores = df[mod_txt_score_col].values\n",
    "                    base_texts = df[base_txt_col].values\n",
    "                    mod_texts = df[mod_txt_col].values\n",
    "\n",
    "                    # Create boolean masks\n",
    "                    model_chos_base_mask = base_txt_scores > mod_txt_scores\n",
    "                    model_chos_mod_mask = base_txt_scores < mod_txt_scores\n",
    "                    # rej_mask = base_txt_scores < mod_txt_scores\n",
    "\n",
    "                    # Extract texts based on masks\n",
    "                    model_txt_dict['chos'].extend(base_texts[model_chos_base_mask])\n",
    "                    model_txt_dict['rej'].extend(mod_texts[model_chos_base_mask])\n",
    "                    model_txt_dict['chos'].extend(mod_texts[model_chos_mod_mask])\n",
    "                    model_txt_dict['rej'].extend(base_texts[model_chos_mod_mask])\n",
    "\n",
    "                # Get alignments and vectorizer\n",
    "                alignments, vectorizer = idp_utils.get_alignments(model_txt_dict, alignment_category='chos', return_vectorizer=True)\n",
    "\n",
    "                alignments['aalScore'] = [langid_utils.detectDialect(t)['aav'] for t in alignments.index]\n",
    "                path = f\"{ALIGNMENTS_TOK_DIR}/{dataset_name}/{prompt_strategy}/{suffix}/{model}.csv\"\n",
    "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "                alignments.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation: document RM score x document AALness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_aal_score(text):\n",
    "    return langid_utils.detectDialect(text)['aav']\n",
    "\n",
    "for rb_execution_config in RB_EXECUTION_CONFIGS:\n",
    "    dataset_name, col_prefix_dict, base_suffix, mod_suffixes, all_suffixes, _ = config_utils.parse_rb_execution_config(rb_execution_config)\n",
    "    prompt_strategy = WO_PROMPT_KEY if dataset_name == DG_DATASET_NAME else W_PROMPT_KEY\n",
    "    col_prefixes = [CHOSEN_KEY] if dataset_name == DG_DATASET_NAME else [CHOSEN_KEY, REJECTED_KEY]\n",
    "    if dataset_name == DG_DATASET_NAME and not HAVE_ALL_DG_DATASETS:\n",
    "        continue\n",
    "    df = DF_DICT[dataset_name]\n",
    "    \n",
    "    # Prepare base text columns\n",
    "    _, base_candidate_suffix = get_prompt_and_candidate_suffixes(\n",
    "        prompt_strategy, base_suffix, base_suffix)\n",
    "    base_txt_cols = [f'{col_prefix_dict[col_prefix]}_{base_candidate_suffix}'\n",
    "                     for col_prefix in col_prefixes]\n",
    "\n",
    "    # Precompute and store 'aalScore's for all texts\n",
    "    all_texts = set()\n",
    "    for suffix in mod_suffixes:\n",
    "        _, mod_candidate_suffix = get_prompt_and_candidate_suffixes(\n",
    "            prompt_strategy, base_suffix, suffix)\n",
    "        mod_txt_cols = [f'{col_prefix_dict[col_prefix]}_{mod_candidate_suffix}'\n",
    "                        for col_prefix in col_prefixes]\n",
    "        # Collect all texts from base and modified columns\n",
    "        for txt_col in base_txt_cols + mod_txt_cols:\n",
    "            all_texts.update(df[txt_col].values)\n",
    "\n",
    "    # Cache 'aalScore's for all unique texts\n",
    "    aal_score_cache = {}\n",
    "    for text in all_texts:\n",
    "        aal_score_cache[text] = get_aal_score(text)\n",
    "\n",
    "    for suffix in mod_suffixes:\n",
    "        _, mod_candidate_suffix = get_prompt_and_candidate_suffixes(\n",
    "            prompt_strategy, base_suffix, suffix)\n",
    "        mod_txt_cols = [f'{col_prefix_dict[col_prefix]}_{mod_candidate_suffix}'\n",
    "                        for col_prefix in col_prefixes]\n",
    "\n",
    "        # Prepare a DataFrame once per suffix\n",
    "        model_txt_df = pd.DataFrame()\n",
    "\n",
    "        # Collect texts\n",
    "        txt_list = []\n",
    "        for base_txt_col, mod_txt_col in zip(base_txt_cols, mod_txt_cols):\n",
    "            txt_list.extend(df[base_txt_col].values)\n",
    "            txt_list.extend(df[mod_txt_col].values)\n",
    "        model_txt_df['txt'] = txt_list\n",
    "\n",
    "        # Map 'aalScore's from the cache\n",
    "        model_txt_df['aalScore'] = model_txt_df['txt'].map(aal_score_cache)\n",
    "\n",
    "        for model in MODELS_DICT:\n",
    "            # Collect scores for the current model\n",
    "            score_list = []\n",
    "            for base_txt_col, mod_txt_col in zip(base_txt_cols, mod_txt_cols):\n",
    "                base_txt_score_col = pandas_utils.get_corresponding_score_col(\n",
    "                    base_txt_col, prompt_strategy, model, dataset_name)\n",
    "                mod_txt_score_col = pandas_utils.get_corresponding_score_col(\n",
    "                    mod_txt_col, prompt_strategy, model, dataset_name)\n",
    "                score_list.extend(df[base_txt_score_col].values)\n",
    "                score_list.extend(df[mod_txt_score_col].values)\n",
    "            model_txt_df['score'] = score_list\n",
    "\n",
    "\n",
    "            path = f\"{ALIGNMENTS_DOC_DIR}/{dataset_name}/{prompt_strategy}/{suffix}/{model}.csv\"\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            model_txt_df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RB (+DG) Datasets Dialect Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_dialect_scores(text):\n",
    "    \"\"\"Calculate dialect scores for a given text.\"\"\"\n",
    "    s = detectDialect(str(text))\n",
    "    return pd.Series({\n",
    "        'aal_score': s['aav'],\n",
    "        'hispanic_score': s['hispanic'], \n",
    "        'white_score': s['white'],\n",
    "        'other_score': s['other']\n",
    "    })\n",
    "\n",
    "def process_text_column(df, txt_col):\n",
    "    \"\"\"Process a single text column and return scores.\"\"\"\n",
    "    scores = df[txt_col].progress_apply(detect_dialect_scores)\n",
    "    scores.columns = [f'{txt_col}${col}' for col in scores.columns]\n",
    "    return scores\n",
    "\n",
    "def analyze_dialect_scores(df, dataset_name, execution_config):\n",
    "    \"\"\"Analyze dialect scores for a dataset and return a DataFrame.\"\"\"\n",
    "    dataset_name, col_prefix_dict, base_suffix, mod_suffixes, all_suffixes, _ = config_utils.parse_rb_execution_config(rb_execution_config)\n",
    "    col_prefixes = [CHOSEN_KEY] if dataset_name == DG_DATASET_NAME else [PROMPT_KEY, CHOSEN_KEY, REJECTED_KEY]\n",
    "    df = DF_DICT[dataset_name]\n",
    "\n",
    "    results = []\n",
    "    for suffix in all_suffixes:\n",
    "        txt_cols = [f'{col_prefix_dict[col_prefix]}_{suffix}' for col_prefix in col_prefixes]\n",
    "        \n",
    "        for txt_col in txt_cols:\n",
    "            df = df.join(process_text_column(df, txt_col))\n",
    "            \n",
    "            scores = {\n",
    "                'Dataset': dataset_name,\n",
    "                'Suffix': suffix,\n",
    "                'Column': txt_col,\n",
    "                'aal': df[f'{txt_col}$aal_score'].mean(),\n",
    "                'hispanic': df[f'{txt_col}$hispanic_score'].mean(),\n",
    "                'white': df[f'{txt_col}$white_score'].mean(),\n",
    "                'other': df[f'{txt_col}$other_score'].mean()\n",
    "            }\n",
    "            results.append(scores)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if not os.path.exists(OUR_DATASETS_DIALECT_ANALYSIS_PATH) or FORCE_RERUN_OUR_DATASETS_DIALECT_ANALYSIS:\n",
    "    all_results = []\n",
    "    for rb_execution_config in RB_EXECUTION_CONFIGS:\n",
    "        dataset_name = rb_execution_config[DATASET_NAME_KEY]\n",
    "        if dataset_name == DG_DATASET_NAME and not HAVE_ALL_DG_DATASETS:\n",
    "            continue\n",
    "        df = DF_DICT[dataset_name]\n",
    "        results = analyze_dialect_scores(df, dataset_name, rb_execution_config)\n",
    "        all_results.extend(results)\n",
    "\n",
    "    our_datasets_dialect_analysis_df = pd.DataFrame(all_results)\n",
    "    our_datasets_dialect_analysis_df.to_csv(OUR_DATASETS_DIALECT_ANALYSIS_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RM Training Datasets Dialect Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Anthropic/hh-rlhf/train/chosen_aav_score, result already exists.\n",
      "Skipping Anthropic/hh-rlhf/train/rejected_aav_score, result already exists.\n",
      "Skipping argilla/dpo-mix-7k/train/chosen_aav_score, result already exists.\n",
      "Skipping argilla/dpo-mix-7k/train/rejected_aav_score, result already exists.\n",
      "Skipping NCSOFT/offsetbias/train/output_1_aav_score, result already exists.\n",
      "Skipping NCSOFT/offsetbias/train/output_2_aav_score, result already exists.\n",
      "Skipping RLHFlow/UltraFeedback-preference-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/UltraFeedback-preference-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping RLHFlow/Helpsteer-preference-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/Helpsteer-preference-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping RLHFlow/HH-RLHF-Helpful-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/HH-RLHF-Helpful-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping RLHFlow/Orca-distibalel-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/Orca-distibalel-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping RLHFlow/Capybara-distibalel-Filter-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/Capybara-distibalel-Filter-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping RLHFlow/CodeUltraFeedback-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/CodeUltraFeedback-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping RLHFlow/UltraInteract-filtered-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/UltraInteract-filtered-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping RLHFlow/PKU-SafeRLHF-30K-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/PKU-SafeRLHF-30K-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping RLHFlow/Argilla-Math-DPO-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/Argilla-Math-DPO-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping RLHFlow/Prometheus2-preference-standard/train/chosen_aav_score, result already exists.\n",
      "Skipping RLHFlow/Prometheus2-preference-standard/train/rejected_aav_score, result already exists.\n",
      "Skipping argilla/OpenHermesPreferences/train/chosen_aav_score, result already exists.\n",
      "Skipping argilla/OpenHermesPreferences/train/rejected_aav_score, result already exists.\n",
      "Skipping openbmb/UltraFeedback/train/instruction_aav_score, result already exists.\n",
      "Skipping openbmb/UltraInteract_pair/train/chosen_aav_score, result already exists.\n",
      "Skipping openbmb/UltraInteract_pair/train/rejected_aav_score, result already exists.\n",
      "Skipping openbmb/UltraSafety/train/completions_aav_score, result already exists.\n",
      "Skipping weqweasdas/preference_dataset_mixture2_and_safe_pku/train/chosen_aav_score, result already exists.\n",
      "Skipping weqweasdas/preference_dataset_mixture2_and_safe_pku/train/rejected_aav_score, result already exists.\n",
      "Skipping llm-blender/Unified-Feedback/train/conv_A_aav_score, result already exists.\n",
      "Skipping llm-blender/Unified-Feedback/train/conv_B_aav_score, result already exists.\n",
      "Skipping stanfordnlp/SHP/train/history_aav_score, result already exists.\n",
      "Skipping stanfordnlp/SHP/train/human_ref_A_aav_score, result already exists.\n",
      "Skipping stanfordnlp/SHP/train/human_ref_B_aav_score, result already exists.\n",
      "Skipping argilla/distilabel-capybara-dpo-7k-binarized/train/chosen_aav_score, result already exists.\n",
      "Skipping argilla/distilabel-capybara-dpo-7k-binarized/train/rejected_aav_score, result already exists.\n",
      "Skipping nvidia/HelpSteer/train/prompt_aav_score, result already exists.\n",
      "Skipping nvidia/HelpSteer/train/response_aav_score, result already exists.\n",
      "Skipping argilla/distilabel-intel-orca-dpo-pairs/train/input_aav_score, result already exists.\n",
      "Skipping argilla/distilabel-intel-orca-dpo-pairs/train/chosen_aav_score, result already exists.\n",
      "Skipping argilla/distilabel-intel-orca-dpo-pairs/train/rejected_aav_score, result already exists.\n",
      "Skipping Intel/orca_dpo_pairs/train/question_aav_score, result already exists.\n",
      "Skipping Intel/orca_dpo_pairs/train/chosen_aav_score, result already exists.\n",
      "Skipping Intel/orca_dpo_pairs/train/rejected_aav_score, result already exists.\n",
      "Skipping allenai/ultrafeedback_binarized_cleaned/train_prefs/chosen_aav_score, result already exists.\n",
      "Skipping allenai/ultrafeedback_binarized_cleaned/train_prefs/rejected_aav_score, result already exists.\n",
      "Skipping allenai/tulu-2.5-preference-data/preference_big_mixture/chosen_aav_score, result already exists.\n",
      "Skipping allenai/tulu-2.5-preference-data/preference_big_mixture/rejected_aav_score, result already exists.\n",
      "Skipping hendrydong/preference_700K/train/chosen_aav_score, result already exists.\n",
      "Skipping hendrydong/preference_700K/train/rejected_aav_score, result already exists.\n",
      "Skipping 0-hero/Matter-0.1/train/conversations_aav_score, result already exists.\n",
      "Skipping allenai/tulu-2.5-preference-data/ultrafeedback_mean_aspects/chosen_aav_score, result already exists.\n",
      "Skipping allenai/tulu-2.5-preference-data/ultrafeedback_mean_aspects/rejected_aav_score, result already exists.\n",
      "Skipping HuggingFaceH4/ultrafeedback_binarized/train_prefs/chosen_aav_score, result already exists.\n",
      "Skipping HuggingFaceH4/ultrafeedback_binarized/train_prefs/rejected_aav_score, result already exists.\n",
      "0-hero/Matter-0.1-7B-boost-DPO-preview 0.0622815514795307\n",
      "[0.0622815514795307]\n",
      "[2253511]\n",
      "\n",
      "stabilityai/stablelm-2-12b-chat 0.058962181966309556\n",
      "[0.05845339092803299, 0.060159886621473195, 0.04946165622192557, 0.05205125443729453]\n",
      "[160800, 160800, 6750, 6750]\n",
      "\n",
      "stabilityai/stablelm-zephyr-3b 0.05483760880834546\n",
      "[0.07549080843255342, 0.07145859165467676, 0.05678161298233802, 0.049425802997751386, 0.05200034842016578]\n",
      "[12859, 12859, 12859, 61135, 61135]\n",
      "\n",
      "weqweasdas/RM-Mistral-7B 0.06872261869037974\n",
      "[0.06664134126263783, 0.07080389611812167]\n",
      "[554903, 554903]\n",
      "\n",
      "NCSOFT/Llama-3-OffsetBias-RM-8B 0.05546834786818887\n",
      "[0.05323307866649819, 0.05386536508769918, 0.0492860641494596, 0.05394220138527577, 0.05652108956403654, 0.06089982616654218, 0.08998786450132887, 0.0944756109192634, 0.06410559509028524, 0.06557265995059716, 0.04404790100890342, 0.04171381860508829, 0.042884913475405316, 0.04362930987410247, 0.05202906028454845, 0.052977340294921335, 0.1091978641406639, 0.10890704714897585, 0.04730363347269412, 0.047730064719657156, 0.03865299236772388, 0.03981936324511974]\n",
      "[8504, 8504, 340025, 340025, 37131, 37131, 115396, 115396, 6926, 6926, 14811, 14811, 50156, 50156, 161927, 161927, 26874, 26874, 2418, 2418, 199760, 199760]\n",
      "\n",
      "allenai/tulu-v2.5-13b-preference-mix-rm 0.07147251702413654\n",
      "[0.06923932521537246, 0.07370570883290062]\n",
      "[259851, 259851]\n",
      "\n",
      "Ray2333/GRM-llama3-8B-distill 0.06481172905318956\n",
      "[0.06268071529028388, 0.06694274281609525]\n",
      "[700000, 700000]\n",
      "\n",
      "Ray2333/Gemma-2B-rewardmodel-baseline 0.06872261869037974\n",
      "[0.06664134126263783, 0.07080389611812167]\n",
      "[554903, 554903]\n",
      "\n",
      "upstage/SOLAR-10.7B-Instruct-v1.0 0.05589173490878333\n",
      "[0.07549080843255342, 0.07145859165467676, 0.05678161298233802, 0.04970313131841652, 0.05445828915614855]\n",
      "[12859, 12859, 12859, 60829, 60829]\n",
      "\n",
      "openbmb/Eurus-RM-7b 0.06537213767137759\n",
      "[0.07316403642776971, 0.0636402197151441, 0.06513078363764359, 0.043622879681881034]\n",
      "[63967, 219522, 219522, 3000]\n",
      "\n",
      "allenai/tulu-2-dpo-7b 0.05071307570895859\n",
      "[0.049425802997751386, 0.05200034842016578]\n",
      "[61135, 61135]\n",
      "\n",
      "sfairXC/FsfairX-LLaMA3-RM-v0.1 0.05548542954860325\n",
      "[0.0492860641494596, 0.05394220138527577, 0.05652108956403654, 0.06089982616654218, 0.08998786450132887, 0.0944756109192634, 0.06410559509028524, 0.06557265995059716, 0.04404790100890342, 0.04171381860508829, 0.042884913475405316, 0.04362930987410247, 0.05202906028454845, 0.052977340294921335, 0.1091978641406639, 0.10890704714897585, 0.04730363347269412, 0.047730064719657156, 0.03865299236772388, 0.03981936324511974]\n",
      "[340025, 340025, 37131, 37131, 115396, 115396, 6926, 6926, 14811, 14811, 50156, 50156, 161927, 161927, 26874, 26874, 2418, 2418, 199760, 199760]\n",
      "\n",
      "allenai/llama-3-tulu-2-8b-uf-mean-rm 0.050119109707758736\n",
      "[0.04737913758387044, 0.05285908183164704]\n",
      "[60908, 60908]\n",
      "\n",
      "Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback 0.07585034593329643\n",
      "[0.0759596820134629, 0.07574100985312995]\n",
      "[884515, 884515]\n",
      "\n",
      "weqweasdas/RM-Gemma-7B 0.07407830179802798\n",
      "[0.05845339092803299, 0.060159886621473195, 0.07316403642776971, 0.06818854687176762, 0.086097669083345, 0.08612856097963517, 0.04019458148046637, 0.03923509819519138, 0.059709959333140154, 0.06657136403643457, 0.07549080843255342, 0.06607695723160342, 0.06216324740541134]\n",
      "[160800, 160800, 63967, 348718, 348718, 348718, 7563, 7563, 35331, 35331, 12859, 12859, 12859]\n",
      "\n",
      "NousResearch/Nous-Hermes-2-Mistral-7B-DPO 0.053730819372095465\n",
      "[0.05270957867033966, 0.05475206007385127]\n",
      "[989490, 989490]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load existing results from JSON if available\n",
    "if os.path.exists(AVG_AAV_SCORES_JSON_PATH) and not FORCE_RERUN_RM_TRAINING_DATASETS_DIALECT_ANALYSIS:\n",
    "    with open(AVG_AAV_SCORES_JSON_PATH, \"r\") as f:\n",
    "        avg_aav_scores_dict = json.load(f)\n",
    "else:\n",
    "    avg_aav_scores_dict = {}\n",
    "\n",
    "def get_dataset(dataset_name, split, save_path, config_name=None):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure directory exists\n",
    "    if not os.path.exists(save_path):\n",
    "        dataset = load_dataset(dataset_name, split=split, name=config_name)\n",
    "        dataset.save_to_disk(save_path)\n",
    "    else:\n",
    "        dataset = Dataset.load_from_disk(save_path)\n",
    "    return dataset\n",
    "\n",
    "def get_aav_score(text):\n",
    "    s = detectDialect(str(text))\n",
    "    return s['aav']\n",
    "\n",
    "# Process each dataset\n",
    "for d_name, d_vals in RM_TRAIN_DATASETS:\n",
    "    config_name = d_vals['config'] if 'config' in d_vals else None\n",
    "    for split in d_vals['splits']:\n",
    "        for col in d_vals['text_cols']:\n",
    "            score_key = f\"{d_name}/{split}/{col}_aav_score\"\n",
    "            \n",
    "            # Skip if the result already exists\n",
    "            if score_key in avg_aav_scores_dict:\n",
    "                print(f\"Skipping {score_key}, result already exists.\")\n",
    "                continue\n",
    "\n",
    "            # Load dataset\n",
    "            dataset_path = os.path.join(RM_TRAINING_DATASETS_DIR, d_name.replace('/', '_'), split)\n",
    "            dataset =  get_dataset(d_name, split, dataset_path, config_name=config_name)\n",
    "\n",
    "            # Limit dataset to a random sample of min(full size, 30k)\n",
    "            max_samples = min(len(dataset), 30000)\n",
    "            random_indices = random.sample(range(len(dataset)), max_samples)\n",
    "            dataset = dataset.select(random_indices)\n",
    "\n",
    "            # Compute AAV scores\n",
    "            score_col = f\"{col}_aav_score\"\n",
    "            dataset = dataset.map(lambda row: {**row, score_col: get_aav_score(row[col])})\n",
    "            aav_scores = [row[score_col] for row in dataset if score_col in row]\n",
    "\n",
    "            # Calculate and store average AAV\n",
    "            avg_aav = sum(aav_scores) / len(aav_scores) if aav_scores else 0\n",
    "            avg_aav_scores_dict[score_key] = avg_aav\n",
    "\n",
    "            # Print result\n",
    "            print(f\"Average 'aav' for {col} in {d_name}/{split}: {avg_aav}\")\n",
    "\n",
    "    # Save all results to JSON\n",
    "    with open(AVG_AAV_SCORES_JSON_PATH, \"w\") as json_file:\n",
    "        json.dump(avg_aav_scores_dict, json_file, indent=4)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "avg_aav_df = pd.DataFrame(list(avg_aav_scores_dict.items()), columns=['Dataset_Score_Column', 'Avg_AAV'])\n",
    "\n",
    "if os.path.exists(MODEL_AVG_AAV_SCORE_JSON_PATH):\n",
    "    with open(MODEL_AVG_AAV_SCORE_JSON_PATH, \"r\") as f:\n",
    "        model_avg_aav_score_dict = json.load(f)\n",
    "else:\n",
    "    model_avg_aav_score_dict = {}\n",
    "\n",
    "# Process each dataset\n",
    "models_set = set()\n",
    "for _, vals in RM_TRAIN_DATASETS:\n",
    "    for model in vals['models']:\n",
    "        models_set.add(model)\n",
    "for model in models_set:\n",
    "    scores = []\n",
    "    lengths = []\n",
    "    for d_name, d_vals in RM_TRAIN_DATASETS:\n",
    "        if model not in d_vals['models']:\n",
    "            continue\n",
    "        config_name = d_vals['config'] if 'config' in d_vals else None\n",
    "        for split in d_vals['splits']:\n",
    "            for col in d_vals['text_cols']:\n",
    "                score_key = f\"{d_name}/{split}/{col}_aav_score\"\n",
    "                scores.append(avg_aav_scores_dict[score_key])\n",
    "                dataset_path = os.path.join(RM_TRAINING_DATASETS_DIR, d_name.replace('/', '_'), split)\n",
    "                dataset = get_dataset(d_name, split, dataset_path, config_name=config_name)\n",
    "                lengths.append(len(dataset))\n",
    "    result = 0\n",
    "    for score, length in zip(scores, lengths):\n",
    "        result += score * length\n",
    "    \n",
    "    model_avg_aav_score_dict[model] = result / sum(lengths)\n",
    "\n",
    "    print(model, model_avg_aav_score_dict[model])\n",
    "    print(scores)\n",
    "    print(lengths)\n",
    "    print()\n",
    "\n",
    "    # Save all results to JSON\n",
    "    with open(MODEL_AVG_AAV_SCORE_JSON_PATH, \"w\") as json_file:\n",
    "        json.dump(model_avg_aav_score_dict, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
