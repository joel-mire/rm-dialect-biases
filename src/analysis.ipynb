{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from utils import pandas_utils, stats_utils, config_utils\n",
    "from constants import * \n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import reduce\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_ALL_DG_DATASETS = config_utils.have_all_datasets()\n",
    "\n",
    "rb_df = pd.read_csv(REWARD_BENCH_PATH)\n",
    "dg_df = pd.read_csv(DEAS_GROENWOLD_PATH)\n",
    "DF_DICT = {\n",
    "   RB_DATASET_NAME: rb_df,\n",
    "   DG_DATASET_NAME: dg_df\n",
    "}\n",
    "# if HAVE_ALL_DG_DATASETS:\n",
    "#    dg_df = pd.read_csv(DEAS_GROENWOLD_PATH)\n",
    "#    DF_DICT[DG_DATASET_NAME] = dg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chos_acc(df, chos_score_col, rej_score_col):\n",
    "    return (df[chos_score_col] > df[rej_score_col]).mean()\n",
    "\n",
    "def mark_significant(val, p_val, alpha=0.05):\n",
    "    \"\"\"Append '*' to the value if the corresponding p-value is below 0.05.\"\"\"\n",
    "    return f\"{round(val, 2)}*\" if p_val < alpha else f\"{round(val, 2)}\"\n",
    "\n",
    "def escape_latex(s: str) -> str:\n",
    "    \"\"\"Escapes special LaTeX characters in a string.\"\"\"\n",
    "    replacements = {\n",
    "        \"&\": r\"\\&\",\n",
    "        \"%\": r\"\\%\",\n",
    "        \"$\": r\"\\$\",\n",
    "        \"#\": r\"\\#\",\n",
    "        \"_\": r\"\\_\",\n",
    "        \"{\": r\"\\{\",\n",
    "        \"}\": r\"\\}\",\n",
    "        \"~\": r\"\\textasciitilde{}\",\n",
    "        \"^\": r\"\\textasciicircum{}\",\n",
    "        \"\\\\\": r\"\\textbackslash{}\",\n",
    "    }\n",
    "    return re.sub(r\"([&%$#_{}~^\\\\])\", lambda m: replacements[m.group()], s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_col_name = 'p'\n",
    "p_holm_col_name = \"$p_{Holm}$\"\n",
    "\n",
    "for rb_execution_config in RB_EXECUTION_CONFIGS:\n",
    "  dataset_name, _, base_suffix, mod_suffixes, _, _ = config_utils.parse_rb_execution_config(rb_execution_config)\n",
    "  if dataset_name == DG_DATASET_NAME:\n",
    "    continue\n",
    "  df = DF_DICT[dataset_name]\n",
    "  prompt_strategy = W_PROMPT_KEY\n",
    "  for mod_suffix in mod_suffixes:\n",
    "    results = {}\n",
    "    \n",
    "    _base_acc_latex_col = f\"Acc_{dataset_name.upper()}-{base_suffix.upper()}\"\n",
    "    base_acc_latex_col = f\"${_base_acc_latex_col}$\"\n",
    "    mod_acc_drop_latex_col = f\"$Acc_{dataset_name.upper()}-{mod_suffix.upper()} - {_base_acc_latex_col}$\"\n",
    "\n",
    "    for model in MODELS_DICT.keys():\n",
    "      # get relevant score col names\n",
    "      base_chos_score_col = pandas_utils.get_score_col_name(model, prompt_strategy, base_suffix, CHOSEN_KEY)\n",
    "      base_rej_score_col = pandas_utils.get_score_col_name(model, prompt_strategy, base_suffix, REJECTED_KEY)\n",
    "      mod_chos_score_col = pandas_utils.get_score_col_name(model, prompt_strategy, mod_suffix, CHOSEN_KEY)\n",
    "      mod_rej_score_col = pandas_utils.get_score_col_name(model, prompt_strategy, mod_suffix, REJECTED_KEY)\n",
    "\n",
    "      # get correctly scored items for base and mod texts\n",
    "      base_correct = df[base_chos_score_col] > df[base_rej_score_col]\n",
    "      mod_correct = df[mod_chos_score_col] > df[mod_rej_score_col]\n",
    "\n",
    "      # construct contingency table for McNemar's test\n",
    "      base_y_mod_y = ((base_correct == True) & (mod_correct == True)).sum()\n",
    "      base_y_mod_n = ((base_correct == True) & (mod_correct == False)).sum()\n",
    "      base_n_mod_y = ((base_correct == False) & (mod_correct == True)).sum()\n",
    "      base_n_mod_n = ((base_correct == False) & (mod_correct == False)).sum()  # Both incorrect\n",
    "      \n",
    "      contingency_table = [[base_y_mod_y, base_y_mod_n],\n",
    "                          [base_n_mod_y, base_n_mod_n]]\n",
    "\n",
    "      # run McNemar's test\n",
    "      mcnemar_result = mcnemar(contingency_table, exact=False, correction=False)\n",
    "\n",
    "      # compute accuracies for base and mod texts\n",
    "      base_acc = get_chos_acc(df, base_chos_score_col, base_rej_score_col)\n",
    "      mod_acc = get_chos_acc(df, mod_chos_score_col, mod_rej_score_col)\n",
    "\n",
    "      # store the results\n",
    "      results[escape_latex(model)] = {\n",
    "          base_acc_latex_col: base_acc,\n",
    "          mod_acc_drop_latex_col: mod_acc - base_acc,\n",
    "          # \"McNemar Stat\": mcnemar_result.statistic,\n",
    "          p_col_name: mcnemar_result.pvalue\n",
    "      }\n",
    "    \n",
    "\n",
    "  results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "  results_df.index.name = \"Reward Model\"\n",
    "\n",
    "  # apply Holm correction to p-values\n",
    "  _, pvals_corrected, _, _ = multipletests(pvals=results_df['p'].values, \n",
    "                                          alpha=0.05, \n",
    "                                          method='holm')\n",
    "  results_df[p_holm_col_name] = pvals_corrected\n",
    "\n",
    "  # add p-value info directly to Acc_{RB-V-PHON} Drop column\n",
    "  results_df[mod_acc_drop_latex_col] = results_df.apply(lambda row: mark_significant(row[mod_acc_drop_latex_col], row[p_holm_col_name]), axis=1)\n",
    "\n",
    "  # drop unused columns\n",
    "  results_df.drop([p_col_name, p_holm_col_name], axis=1, inplace=True)\n",
    "  results_df = results_df.round(2)\n",
    "\n",
    "  results_df = results_df.sort_values(by=mod_acc_drop_latex_col, ascending=False)\n",
    "\n",
    "  # Start constructing the LaTeX table and save it to the file\n",
    "  with open(f'{RESULTS_DIR}/rq1_acc_comparison_{dataset_name}_{base_suffix}_{mod_suffix}.tex', 'w') as f:\n",
    "      f.write('\\\\begin{table*}[htp]\\n')\n",
    "      f.write('\\\\begin{center}\\n')\n",
    "      f.write('\\\\small\\n')\n",
    "      f.write('\\\\begin{tabular}{lrr}\\n')\n",
    "      f.write('\\\\toprule\\n')\n",
    "      f.write(f'Reward Model & {base_acc_latex_col} & {mod_acc_drop_latex_col} \\\\\\\\\\n')\n",
    "      f.write('\\\\midrule\\n')\n",
    "      \n",
    "      # Loop through each row and write the table contents\n",
    "      for idx, row in results_df.iterrows():\n",
    "          model_name = idx  # Model name (already LaTeX-escaped)\n",
    "          acc_orig = row[base_acc_latex_col]\n",
    "          acc_drop = row[mod_acc_drop_latex_col]\n",
    "          acc_orig_formatted = f\"{acc_orig:.2f}\"  # Ensure two decimal places\n",
    "          f.write(f\"{model_name} & {acc_orig_formatted} & {acc_drop} \\\\\\\\\\n\")\n",
    "      \n",
    "      # Finish the table\n",
    "      f.write('\\\\bottomrule\\n')\n",
    "      f.write('\\\\end{tabular}\\n')\n",
    "      f.write('\\\\normalsize\\n')\n",
    "      f.write('\\\\end{center}\\n')\n",
    "      f.write('\\\\end{table*}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table has been written to /home/jmire/rm-dialect-biases/results/rq2_combined_results.tex\n"
     ]
    }
   ],
   "source": [
    "def compute_effect_size_results(rb_execution_config):\n",
    "    dataset_name, _, base_suffix, mod_suffixes, _, _ = config_utils.parse_rb_execution_config(rb_execution_config)\n",
    "    df = DF_DICT[dataset_name]\n",
    "    mod_suffix = mod_suffixes[0]    # Assuming only one mod suffix for now\n",
    "    prompt_strategy = WO_PROMPT_KEY if dataset_name == DG_DATASET_NAME else W_PROMPT_KEY    # only look at the most relevant prompt strategy for each dataset\n",
    "\n",
    "    n_samples = df.shape[0]\n",
    "    pair_id = np.arange(n_samples)\n",
    "    data_base = pd.DataFrame({'pair_id': pair_id, 'condition': 'base'})\n",
    "    data_mod = pd.DataFrame({'pair_id': pair_id, 'condition': 'mod'})\n",
    "    measure_cols = list(MODELS_DICT.keys())\n",
    "    for model in measure_cols:\n",
    "        base_col = pandas_utils.get_score_col_name(model, prompt_strategy, base_suffix, CHOSEN_KEY)\n",
    "        mod_col = pandas_utils.get_score_col_name(model, prompt_strategy, mod_suffix, CHOSEN_KEY)\n",
    "        data_base[model] = df[base_col].values\n",
    "        data_mod[model] = df[mod_col].values\n",
    "\n",
    "    combined_data = pd.concat([data_base, data_mod], ignore_index=True)\n",
    "\n",
    "    effect_size_df = stats_utils.ttestSummaries(\n",
    "        df=combined_data,\n",
    "        condition_col='condition',\n",
    "        measure_cols=measure_cols,\n",
    "        paired='pair_id',\n",
    "        fillna=None\n",
    "    )\n",
    "\n",
    "    return dataset_name, effect_size_df\n",
    "\n",
    "def compute_correlation_results(rb_execution_config):\n",
    "    dataset_name, _, _, mod_suffixes, _, _ = config_utils.parse_rb_execution_config(rb_execution_config)\n",
    "    mod_suffix = mod_suffixes[0]    # Assuming only one mod suffix for now\n",
    "    prompt_strategy = WO_PROMPT_KEY if dataset_name == DG_DATASET_NAME else W_PROMPT_KEY    # only look at the most relevant prompt strategy for each dataset\n",
    "\n",
    "    model_correlations = {}\n",
    "    for model in MODELS_DICT.keys():\n",
    "        path = f\"{ALIGNMENTS_DOC_DIR}/{dataset_name}/{prompt_strategy}/{mod_suffix}/{model}.csv\"\n",
    "        alignments_tok_df = pd.read_csv(path)\n",
    "        correl_result = stats_utils.correlSummary(alignments_tok_df, 'score', 'aalScore')\n",
    "        model_correlations[model] = correl_result\n",
    "\n",
    "    return dataset_name, model_correlations\n",
    "\n",
    "def apply_holm_correction(correl_results_df, dataset_names):\n",
    "    for dataset_name in dataset_names:\n",
    "        pvalue_col = f'{dataset_name}_pvalue'\n",
    "        correl_col = f'{dataset_name}_correl'\n",
    "        pvals = correl_results_df[pvalue_col].values\n",
    "        _, pvals_corrected, _, _ = multipletests(pvals=pvals, alpha=0.05, method='holm')\n",
    "        correl_results_df[correl_col] = [mark_significant(corr, p_holm) for corr, p_holm in zip(correl_results_df[correl_col], pvals_corrected)]\n",
    "        correl_results_df.drop(columns=[pvalue_col], inplace=True)\n",
    "    return correl_results_df\n",
    "\n",
    "def prepare_effect_size_dataframe(effect_size_results):\n",
    "    d_dfs = []\n",
    "    for dataset_name, effect_size_df in effect_size_results.items():\n",
    "        effect_size_df[\"d\"] = effect_size_df.apply(lambda row: mark_significant(row[\"d\"], row['p_holm']), axis=1)\n",
    "        d_results = effect_size_df[['d']].copy()\n",
    "        d_results.columns = pd.MultiIndex.from_tuples([('d', dataset_name.upper())])\n",
    "        d_results.index.name = 'model'\n",
    "        d_dfs.append(d_results)\n",
    "    combined_d_df = reduce(lambda left, right: left.join(right, how='outer'), d_dfs)\n",
    "    return combined_d_df\n",
    "\n",
    "def prepare_correlation_dataframe(correl_results_df, dataset_names):\n",
    "    correl_dfs = []\n",
    "    for dataset_name in dataset_names:\n",
    "        correl_col = f'{dataset_name}_correl'\n",
    "        correl_df = correl_results_df[[correl_col]].copy()\n",
    "        correl_df.columns = pd.MultiIndex.from_tuples([('correl', dataset_name)])\n",
    "        correl_df.index.name = 'model'\n",
    "        correl_dfs.append(correl_df)\n",
    "    combined_correl_df = pd.concat(correl_dfs, axis=1)\n",
    "    return combined_correl_df\n",
    "\n",
    "def generate_latex_table(combined_results, output_file_path):\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write('\\\\begin{table*}[htp]\\n')\n",
    "        f.write('\\\\centering\\n')\n",
    "        f.write('\\\\small\\n')\n",
    "        f.write('\\\\begin{tabular}{lcc|cc}\\n')\n",
    "        f.write('\\\\toprule\\n')\n",
    "        # First header row\n",
    "        f.write(' & \\\\multicolumn{2}{c}{\\\\textbf{Effect Size (d)}} & \\\\multicolumn{2}{c}{\\\\textbf{Correlation (r)}} \\\\\\\\\\n')\n",
    "        f.write('\\\\cmidrule(lr){2-3} \\\\cmidrule(lr){4-5}\\n')\n",
    "        # Second header row\n",
    "        f.write(f'\\\\textbf{{Model}} & \\\\textbf{{RB}} & \\\\textbf{{DG}} & \\\\textbf{{RB}} & \\\\textbf{{DG}} \\\\\\\\\\n')\n",
    "        f.write('\\\\midrule\\n')\n",
    "        # Loop through each row and write table contents\n",
    "        for idx, row in combined_results.iterrows():\n",
    "            model_name = escape_latex(str(idx))\n",
    "            # Extract values, handling missing data\n",
    "            d_RB = row.get(('d', 'RB'), '--')\n",
    "            d_DG = row.get(('d', 'DG'), '--')\n",
    "            correl_RB = row.get(('correl', 'RB'), '--')\n",
    "            correl_DG = row.get(('correl', 'DG'), '--')\n",
    "            # Write the row to the LaTeX file with 'RB' before 'DG'\n",
    "            f.write(f\"{model_name} & {d_RB} & {d_DG} & {correl_RB} & {correl_DG} \\\\\\\\\\n\")\n",
    "        # Finish the table\n",
    "        f.write('\\\\bottomrule\\n')\n",
    "        f.write('\\\\end{tabular}\\n')\n",
    "        f.write('\\\\normalsize\\n')\n",
    "        f.write('\\\\caption{Combined Effect Size and Correlation Results by Model and Dataset}\\n')\n",
    "        f.write('\\\\label{tab:combined_results}\\n')\n",
    "        f.write('\\\\end{table*}\\n')\n",
    "\n",
    "# Updated variable names for clarity and accuracy\n",
    "effect_size_results = {}\n",
    "for rb_execution_config in [RB_EXECUTION_CONFIGS[1], RB_EXECUTION_CONFIGS[0]]:\n",
    "    dataset_name, effect_size_df = compute_effect_size_results(rb_execution_config)\n",
    "    effect_size_results[dataset_name] = effect_size_df\n",
    "\n",
    "correlations = {}\n",
    "for rb_execution_config in RB_EXECUTION_CONFIGS:\n",
    "    dataset_name, model_correlations = compute_correlation_results(rb_execution_config)\n",
    "    correlations[dataset_name] = model_correlations\n",
    "\n",
    "# Build a dictionary for correlation results\n",
    "correl_results_dict = {model: {} for model in MODELS_DICT.keys()}\n",
    "for dataset_name, model_correlations in correlations.items():\n",
    "    for model, correl_result in model_correlations.items():\n",
    "        correl_results_dict[model][f'{dataset_name.upper()}_correl'] = correl_result['r']\n",
    "        correl_results_dict[model][f'{dataset_name.upper()}_pvalue'] = correl_result['p']\n",
    "correl_results_df = pd.DataFrame.from_dict(correl_results_dict, orient='index')\n",
    "\n",
    "dataset_names = ['DG', 'RB']\n",
    "\n",
    "correl_results_df = apply_holm_correction(correl_results_df, dataset_names)\n",
    "\n",
    "combined_d_df = prepare_effect_size_dataframe(effect_size_results)\n",
    "combined_correl_df = prepare_correlation_dataframe(correl_results_df, dataset_names)\n",
    "\n",
    "combined_results = combined_d_df.join(combined_correl_df, how='outer')\n",
    "\n",
    "combined_results = combined_results.sort_values(by=('d', 'RB'), ascending=False)\n",
    "\n",
    "output_file_path = f\"{RESULTS_DIR}/rq2_combined_results.tex\"\n",
    "generate_latex_table(combined_results, output_file_path)\n",
    "\n",
    "print(f\"LaTeX table has been written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-level Analysis (not in paper)\n",
    "\n",
    "# Create a dictionary to store results for each model\n",
    "results_dict = {model: {} for model in MODELS_DICT.keys()}\n",
    "\n",
    "for rb_execution_config in RB_EXECUTION_CONFIGS:\n",
    "    dataset_name, _, base_suffix, mod_suffixes, _, _ = config_utils.parse_rb_execution_config(rb_execution_config)\n",
    "    df = DF_DICT[dataset_name]\n",
    "    mod_suffix = mod_suffixes[0]\n",
    "    prompt_strategy = WO_PROMPT_KEY if dataset_name == DG_DATASET_NAME else W_PROMPT_KEY    # only look at the most relevant prompt strategy for each dataset\n",
    "\n",
    "    for model in MODELS_DICT.keys():\n",
    "        path = f\"{ALIGNMENTS_TOK_DIR}/{dataset_name}/{prompt_strategy}/{mod_suffix}/{model}.csv\"\n",
    "        alignments_tok_df = pd.read_csv(path)\n",
    "        correl_result = stats_utils.correlSummary(alignments_tok_df, 'chos', 'aalScore')\n",
    "        \n",
    "        prefix = dataset_name.lower()  # 'dg' or 'rb'\n",
    "        results_dict[model][f'{prefix}_correl'] = correl_result['r']\n",
    "        results_dict[model][f'{prefix}_pvalue'] = correl_result['p']\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "_, dg_pvals_corrected, _, _ = multipletests(pvals=results_df['dg_pvalue'].values, alpha=0.05, method='holm')\n",
    "_, rb_pvals_corrected, _, _ = multipletests(pvals=results_df['rb_pvalue'].values, alpha=0.05, method='holm')\n",
    "\n",
    "results_df['dg_correl'] = [mark_significant(corr, p_holm) \n",
    "                          for corr, p_holm in zip(results_df['dg_correl'], dg_pvals_corrected)]\n",
    "results_df['rb_correl'] = [mark_significant(corr, p_holm) \n",
    "                          for corr, p_holm in zip(results_df['rb_correl'], rb_pvals_corrected)]\n",
    "\n",
    "results_df = results_df[['dg_correl', 'rb_correl']]\n",
    "\n",
    "results_df.to_latex(f\"{RESULTS_DIR}/rq2_token_level_results.tex\", escape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table has been written to /home/jmire/rm-dialect-biases/results/rq3_mirroring_table.tex\n"
     ]
    }
   ],
   "source": [
    "df = rb_df\n",
    "\n",
    "measure_cols = list(MODELS_DICT.keys())\n",
    "\n",
    "n = df.shape[0]\n",
    "pair_id = np.arange(n)\n",
    "\n",
    "data_mirror_base_prompt = pd.DataFrame({'pair_id': pair_id, 'condition': 'mirror'})\n",
    "data_non_mirror_base_prompt = pd.DataFrame({'pair_id': pair_id, 'condition': 'non_mirror'})\n",
    "\n",
    "data_mirror_mod_prompt = pd.DataFrame({'pair_id': pair_id, 'condition': 'mirror'})\n",
    "data_non_mirror_mod_prompt = pd.DataFrame({'pair_id': pair_id, 'condition': 'non_mirror'})\n",
    "\n",
    "base_suffix = 'orig'\n",
    "mod_suffix = 'vpAAL'\n",
    "\n",
    "for model in MODELS_DICT.keys():\n",
    "    # Base prompt \n",
    "    base_prompt_mod_candidate_score_col = pandas_utils.get_score_col_name(model, W_BASE_PROMPT_MOD_CANDIDATE_KEY, mod_suffix, CHOSEN_KEY)\n",
    "    base_prompt_base_candidate_score_col = pandas_utils.get_score_col_name(model, W_PROMPT_KEY, base_suffix, CHOSEN_KEY)\n",
    "\n",
    "    data_mirror_base_prompt[model] = df[base_prompt_base_candidate_score_col].values\n",
    "    data_non_mirror_base_prompt[model] = df[base_prompt_mod_candidate_score_col].values\n",
    "\n",
    "    # Modified prompt\n",
    "    mod_prompt_base_candidate_score_col = pandas_utils.get_score_col_name(model, W_MOD_PROMPT_BASE_CANDIDATE_KEY, mod_suffix, CHOSEN_KEY)\n",
    "    mod_prompt_mod_candidate_score_col = pandas_utils.get_score_col_name(model, W_PROMPT_KEY, mod_suffix, CHOSEN_KEY)\n",
    "\n",
    "    data_mirror_mod_prompt[model] = df[mod_prompt_mod_candidate_score_col].values\n",
    "    data_non_mirror_mod_prompt[model] = df[mod_prompt_base_candidate_score_col].values\n",
    "\n",
    "# Base prompt\n",
    "base_prompt_combined_df = pd.concat([data_non_mirror_base_prompt, data_mirror_base_prompt], ignore_index=True)\n",
    "base_ttest_df = stats_utils.ttestSummaries(\n",
    "    base_prompt_combined_df,\n",
    "    condition_col='condition',\n",
    "    measure_cols=measure_cols,\n",
    "    paired='pair_id'\n",
    ")\n",
    "\n",
    "# Modified prompt\n",
    "mod_prompt_combined_df = pd.concat([data_non_mirror_mod_prompt, data_mirror_mod_prompt], ignore_index=True)\n",
    "mod_ttest_df = stats_utils.ttestSummaries(\n",
    "    mod_prompt_combined_df,\n",
    "    condition_col='condition',\n",
    "    measure_cols=measure_cols,\n",
    "    paired='pair_id'\n",
    ")\n",
    "\n",
    "final_display_df = pd.DataFrame(index=mod_ttest_df.index)\n",
    "final_display_df['Cohen\\'s d (Mod Prompt)'] = [\n",
    "    mark_significant(d, p_val)\n",
    "    for d, p_val in zip(mod_ttest_df['d'], mod_ttest_df['p_holm'])\n",
    "]\n",
    "final_display_df['Cohen\\'s d (Base Prompt)'] = [\n",
    "    mark_significant(d, p_val)\n",
    "    for d, p_val in zip(base_ttest_df['d'], base_ttest_df['p_holm'])\n",
    "]\n",
    "\n",
    "# sort by effect sizes for mod prompt col\n",
    "final_display_df['d_mod_prompt'] = mod_ttest_df['d']\n",
    "final_display_df.sort_values(by='d_mod_prompt', ascending=True, inplace=True)\n",
    "final_display_df.drop(columns=['d_mod_prompt'], inplace=True)\n",
    "\n",
    "final_display_df.reset_index(inplace=True)\n",
    "final_display_df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "\n",
    "output_file = os.path.join(RESULTS_DIR, 'rq3_mirroring_table.tex')\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write('\\\\begin{table*}[htp]\\n')\n",
    "    f.write('\\\\centering\\n')\n",
    "    f.write('\\\\small\\n')\n",
    "    \n",
    "    num_columns = len(final_display_df.columns)\n",
    "\n",
    "    column_alignment = 'lcc|cc'\n",
    "    f.write(f'\\\\begin{{tabular}}{{{column_alignment}}}\\n')\n",
    "    f.write('\\\\toprule\\n')\n",
    "    \n",
    "    headers = final_display_df.columns.tolist()\n",
    "    escaped_headers = [escape_latex(h) for h in headers]\n",
    "    header_line = ' & '.join(escaped_headers) + ' \\\\\\\\\\n'\n",
    "    f.write(header_line)\n",
    "    f.write('\\\\midrule\\n')\n",
    "\n",
    "    for idx, row in final_display_df.iterrows():\n",
    "        escaped_row = [escape_latex(str(value)) for value in row]\n",
    "        row_line = ' & '.join(escaped_row) + ' \\\\\\\\\\n'\n",
    "        f.write(row_line)\n",
    "    \n",
    "    f.write('\\\\bottomrule\\n')\n",
    "    f.write('\\\\end{tabular}\\n')\n",
    "    f.write('\\\\normalsize\\n')\n",
    "    \n",
    "    f.write('\\\\caption{Mirroring comparison}\\n')\n",
    "    f.write('\\\\label{tab:combined_results}\\n')\n",
    "    f.write('\\\\end{table*}\\n')\n",
    "\n",
    "print(f\"LaTeX table has been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dialect Status of the RB (+DG) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(results, output_path):\n",
    "    \"\"\"Generate a LaTeX table from results.\"\"\"\n",
    "    # Prepare data for tabulate with proper case for headers\n",
    "    headers = ['Dataset', 'Suffix', 'Column', 'AAL', 'Hispanic', 'White', 'Other']\n",
    "    \n",
    "    # Create mapping for case-insensitive lookup\n",
    "    case_map = {'AAL': 'aal', 'Hispanic': 'hispanic', 'White': 'white', 'Other': 'other'}\n",
    "    \n",
    "    table_data = []\n",
    "    for i, row in results.iterrows():\n",
    "        formatted_row = [\n",
    "            row['Dataset'],\n",
    "            row['Suffix'],\n",
    "            row['Column']\n",
    "        ]\n",
    "        # Add score values using case mapping\n",
    "        for header in headers[3:]:  # Skip first 3 columns (Dataset, Suffix, Column)\n",
    "            value = row[case_map[header]]\n",
    "            formatted_row.append(f\"{value:.2f}\")\n",
    "        table_data.append(formatted_row)\n",
    "    \n",
    "    latex_table = tabulate(table_data, headers=headers, tablefmt='latex_booktabs', floatfmt='.2f')\n",
    "    \n",
    "    # Add LaTeX table formatting\n",
    "    latex_table = (\n",
    "        \"\\\\begin{table}[htbp]\\n\"\n",
    "        \"\\\\centering\\n\"\n",
    "        \"\\\\caption{Dialect Analysis Results}\\n\"\n",
    "        \"\\\\label{tab:dialect-analysis}\\n\"\n",
    "        + latex_table +\n",
    "        \"\\n\\\\end{table}\"\n",
    "    )\n",
    "    \n",
    "    # Write to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(latex_table)\n",
    "\n",
    "our_dataset_dialect_analysis_df = pd.read_csv(OUR_DATASETS_DIALECT_ANALYSIS_PATH)\n",
    "generate_latex_table(our_dataset_dialect_analysis_df, f\"{RESULTS_DIR}/app_our_datasets_dialect_analysis.tex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
