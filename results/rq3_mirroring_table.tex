\begin{table*}[htp]
\centering
\small
\begin{tabular}{lcc|cc}
\toprule
Model & Cohen's d (Mod Prompt) & Cohen's d (Base Prompt) \\
\midrule
openbmb/Eurus-RM-7b & -0.85* & 0.96* \\
weqweasdas/RM-Mistral-7B & -0.75* & 0.86* \\
Ray2333/GRM-llama3-8B-distill & -0.72* & 0.82* \\
allenai/llama-3-tulu-2-8b-uf-mean-rm & -0.72* & 0.79* \\
internlm/internlm2-20b-reward & -0.69* & 0.76* \\
Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback & -0.62* & 0.72* \\
sfairXC/FsfairX-LLaMA3-RM-v0.1 & -0.6* & 0.65* \\
NCSOFT/Llama-3-OffsetBias-RM-8B & -0.58* & 0.65* \\
NousResearch/Nous-Hermes-2-Mistral-7B-DPO & -0.55* & 0.62* \\
allenai/tulu-v2.5-13b-preference-mix-rm & -0.54* & 0.57* \\
0-hero/Matter-0.1-7B-boost-DPO-preview & -0.54* & 0.54* \\
upstage/SOLAR-10.7B-Instruct-v1.0 & -0.47* & 0.47* \\
allenai/tulu-2-dpo-7b & -0.45* & 0.4* \\
internlm/internlm2-1\_8b-reward & -0.42* & 0.5* \\
Qwen/Qwen1.5-7B-Chat & -0.41* & 0.39* \\
CIR-AMS/BTRM\_Qwen2\_7b\_0613 & -0.37* & 0.43* \\
Ray2333/Gemma-2B-rewardmodel-baseline & -0.34* & 0.41* \\
\bottomrule
\end{tabular}
\normalsize
\caption{Mirroring comparison}
\label{tab:combined_results}
\end{table*}
